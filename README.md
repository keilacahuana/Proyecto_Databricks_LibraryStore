# Proyecto_Databricks_LibraryStore
Proyecto final curso IngenierÃ­a de datos e IA con Databricks usando data de Northern Lights Book Co , una cadena de librerias medianas en Calgary, Alberta

# ğŸ“š Northern Lights Book Payroll ETL Pipeline
### Arquitectura Medallion en Azure Databricks

[![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](https://databricks.com/)
[![Azure](https://img.shields.io/badge/Azure-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)](https://azure.microsoft.com/)
[![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)](https://spark.apache.org/)
[![Delta Lake](https://img.shields.io/badge/Delta_Lake-00ADD8?style=for-the-badge&logo=delta&logoColor=white)](https://delta.io/)
[![Databricks Dashboards](https://img.shields.io/badge/Databricks Dashboards-F2C81?style=for-the-badge&logo=databricks&logoColor=black)](https://databricks.com/)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub_Actions-2088FF?style=for-the-badge&logo=github-actions&logoColor=white)](https://github.com/features/actions)

*Pipeline automatizado de datos para anÃ¡lisis de pagos por comisiones y ventas de una cadena de librerias con arquitectura de tres capas y despliegue continuo*

## ğŸ¯ DescripciÃ³n

Pipeline ETL enterprise-grade que transforma datos crudos de empleados y pagos de una cadena de librerias en diferentes periodos y tiendas fisicas y almacenamiento, implementando la **Arquitectura Medallion** (Bronze-Silver-Gold) en Azure Databricks con **CI/CD completo** y **Delta Lake** para garantizar consistencia ACID.

### âœ¨ CaracterÃ­sticas Principales

- ğŸ”„ **ETL Automatizado** - Pipeline completo con despliegue automÃ¡tico via GitHub Actions
- ğŸ—ï¸ **Arquitectura Medallion** - SeparaciÃ³n clara de capas Bronze â†’ Silver â†’ Gold
- ğŸ“Š **Modelo Dimensional** - Star Schema optimizado para anÃ¡lisis de negocio
- ğŸš€ **CI/CD Integrado** - Deploy automÃ¡tico en cada push a master
- ğŸ“ˆ **Databricks Dashboards** - VisualizaciÃ³n

---

## ğŸ›ï¸ Arquitectura

### Flujo de Datos

```
ğŸ“„ CSV (Raw Data)
    â†“
ğŸ¥‰ Bronze Layer (Ingesta sin transformaciÃ³n)
    â†“
ğŸ¥ˆ Silver Layer (Limpieza + Modelo Dimensional)
    â†“
ğŸ¥‡ Gold Layer (Agregaciones de Negocio)
    â†“
ğŸ“Š Databricks Dashboards (VisualizaciÃ³n)
```

[![Arquitectura](https://blog.bismart.com/hs-fs/hubfs/Arquitectura_Medallion_Pasos.jpg?width=1754&height=656&name=Arquitectura_Medallion_Pasos.jpg)](https://blog.bismart.com/arquitectura-medallion-data-lakehouse)


### ğŸ“¦ Capas del Pipeline

<table>
<tr>
<td width="33%" valign="top">

#### ğŸ¥‰ Bronze Layer
**PropÃ³sito**: Zona de aterrizaje

**Tablas**: 
- `employees` 
- `payroll`


</td>
<td width="33%" valign="top">

#### ğŸ¥ˆ Silver Layer
**PropÃ³sito**: Modelo dimensional

**Tablas**:
- `payroll_transformed`

</td>
<td width="33%" valign="top">

#### ğŸ¥‡ Gold Layer
**PropÃ³sito**: Analytics-ready

**Tablas**:
- golden_payroll_partitioned        : Monto total en payroll agrupado por tipo de local y aÃ±o (Ãºltimos 2 aÃ±os)


</td>
</tr>
</table>

---

## ğŸ“ Estructura del Proyecto

```
etl-library/
â”‚
â”œâ”€â”€ ğŸ“‚ .github/
â”‚   â””â”€â”€ ğŸ“‚ workflows/
â”‚       â””â”€â”€ ğŸ“„ deploy-notebook.yml    # Pipeline CI/CD deploy dev to produccion workspace databricks
â”œâ”€â”€ ğŸ“‚ proceso/
â”‚   â”œâ”€â”€ ğŸ Ingest_employees_data.py           # Bronze layer
â”‚   â”œâ”€â”€ ğŸ Ingest_payroll_data.py             # Bronze Layer
â”‚   â”œâ”€â”€ ğŸ Load.py                            # Silver Layer
â”‚   â”œâ”€â”€ ğŸ Transform.py                       # Golden Layer
â”œâ”€â”€ ğŸ“‚ scripts/
|   â”œâ”€â”€ ğŸ Preparacion_Ambiente.py            # Create Schema, Tables, External location
â”œâ”€â”€ ğŸ“‚ seguridad/
|   â”œâ”€â”€ ğŸ Grants-Medallion.py                # Sql Grant
â”œâ”€â”€ ğŸ“‚ ReversiÃ³n/
|   â”œâ”€â”€ ğŸ Revoke-Medallion.py                # Revoke permissions
â”œâ”€â”€ ğŸ“‚ dashboard/                             # Databricks Dashboards 
â””â”€â”€ ğŸ“„ README.md
```

---

## ğŸ› ï¸ TecnologÃ­as

<div align="center">

| TecnologÃ­a | PropÃ³sito |
|:----------:|:----------|
| ![Databricks](https://img.shields.io/badge/Azure_Databricks-FF3621?style=flat-square&logo=databricks&logoColor=white) | Motor de procesamiento distribuido Spark |
| ![Delta Lake](https://img.shields.io/badge/Delta_Lake-00ADD8?style=flat-square&logo=delta&logoColor=white) | Storage layer con ACID transactions |
| ![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=flat-square&logo=apache-spark&logoColor=white) | Framework de transformaciÃ³n de datos |
| ![ADLS](https://img.shields.io/badge/ADLS_Gen2-0078D4?style=flat-square&logo=microsoft-azure&logoColor=white) | Data Lake para almacenamiento persistente |
| ![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-2088FF?style=flat-square&logo=github-actions&logoColor=white) | AutomatizaciÃ³n CI/CD |
| ![Databricks Dashboards](https://img.shields.io/badge/Databricks Dashboards-F2C81?style=for-the-badge&logo=databricks&logoColor=black) |  VisualizaciÃ³n |

</div>

---

## âš™ï¸ Requisitos Previos

- â˜ï¸ Cuenta de Azure con acceso a Databricks
- ğŸ’» Workspace de Databricks configurado
- ğŸ–¥ï¸ Cluster activo (nombre: `DS_Cluster`)
- ğŸ™ Cuenta de GitHub con permisos de administrador
- ğŸ“¦ Azure Data Lake Storage Gen2 configurado
- ğŸ“Š Power BI Desktop (opcional para visualizaciÃ³n)

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1ï¸âƒ£ Clonar el Repositorio

```bash
git clone https://github.com/keilacahuana/Proyecto_Databricks_LibraryStore.git
cd project-databricks
```

### 2ï¸âƒ£ Configurar Databricks Token

1. Ir a Databricks Workspace
2. **User Settings** â†’ **Developer** â†’ **Access Tokens**
3. Click en **Generate New Token**
4. Configurar:
   - **Comment**: `GitHub CI/CD`
   - **Lifetime**: `90 days`
5. âš ï¸ Copiar y guardar el token

### 3ï¸âƒ£ Configurar GitHub Secrets

En tu repositorio: **Settings** â†’ **Secrets and variables** â†’ **Actions**

| Secret Name | Valor Ejemplo |
|------------|---------------|
| `DATABRICKS_HOST` | `https://adb-xxxxx.azuredatabricks.net` |
| `DATABRICKS_TOKEN` | `dapi_xxxxxxxxxxxxxxxx` |

### 4ï¸âƒ£ Verificar Storage Configuration

```python
storage_path = "abfss://raw@adlbsdb1011.dfs.core.windows.net/"
```

<div align="center">

âœ… **Â¡ConfiguraciÃ³n completa!**

</div>

---

## ğŸ’» Uso

### ğŸ”„ Despliegue AutomÃ¡tico (Recomendado)

```bash
git add .
git commit -m "âœ¨ feat: mejoras en pipeline"
git push origin master
```

**GitHub Actions ejecutarÃ¡**:
- ğŸ“¤ Deploy de notebooks a `/main`
- ğŸ”§ CreaciÃ³n del workflow `WF_LIBRARY_PAYROLL`
- â–¶ï¸ EjecuciÃ³n completa:  Bronze â†’ Silver â†’ Gold
- ğŸ“§ Notificaciones de resultados

### ğŸ–±ï¸ Despliegue Manual desde GitHub

1. Ir al tab **Actions** en GitHub
2. Seleccionar **Deploy ETL Apple Sales And Warranty**
3. Click en **Run workflow**
4. Seleccionar rama `main`
5. Click en **Run workflow**

### ğŸ”§ EjecuciÃ³n Local en Databricks

Navegar a `main` y ejecutar en orden:

```
- Preparacion_Ambiente.py         â†’ Crear esquema
- Ingest_employees_data.py        â†’ Bronze Layer
- Ingest_payroll_data.py          â†’ Bronze Layer
- Load.py                         â†’ Silver Layer
- Transform.py                    â†’ Gold Layer
```

---


## ğŸ”„ CI/CD

### Pipeline de GitHub Actions

```yaml
Workflow: Deploy ETL Library
â”œâ”€â”€ Deploy notebooks â†’ /main
â”œâ”€â”€ Eliminar workflow antiguo (si existe)
â”œâ”€â”€ Buscar cluster configurado
â”œâ”€â”€ Crear nuevo workflow con 5 tareas
â”œâ”€â”€ Ejecutar pipeline automÃ¡ticamente
â””â”€â”€ Monitorear y notificar resultados
```

### ğŸ”„  Workflow Databricks
![Texto descriptivo](DB_WORKFLOW.png)
```


â° Schedule: Diario 8:00 AM (Lima)
â±ï¸ Timeout total: 3 min
 ğŸ”’ Max concurrent runs: 1
â° Notificaciones: 
      success: keila19.1307@gmail.com
      failed:  keila19.1307@gmail.com
```

---

## ğŸ“ˆ Dashboards
https://github.com/keilacahuana/Proyecto_Databricks_LibraryStore/tree/dev/dashboard

## ğŸ” Monitoreo

### En Databricks

**Workflows**:
- Ir a **Workflows** en el menÃº lateral
- Buscar `WF_LIBRARY_PAYROLL`
- Ver historial de ejecuciones

**Logs por Tarea**:
- Click en una ejecuciÃ³n especÃ­fica
- Click en cada tarea para ver logs detallados
- Revisar stdout/stderr en caso de errores

### En GitHub Actions

- Tab **Actions** del repositorio
- Ver historial de workflows
- Click en ejecuciÃ³n especÃ­fica para detalles
- Revisar logs de cada step

---

## ğŸ‘¤ Autor

<div align="center">

### Keila Cahuana Alcantara

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/keila-cahuana-alcantara-304130194/)
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/keilacahuana)

**Data Engineering** | **Azure Databricks** | **Delta Lake** | **CI/CD**

</div>

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

<div align="center">

**Proyecto**: Data Engineering - Arquitectura Medallion  
**TecnologÃ­a**: Azure Databricks + Delta Lake + CI/CD  
**Ãšltima actualizaciÃ³n**: 2025


</div>